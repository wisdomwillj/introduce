{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff5a90a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     769|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sparkSession=SparkSession.builder.appName(\"pyspark-sql-test\").getOrCreate()\n",
    "df = sparkSession.read.csv('hdfs://localhost:9000/user/hive/warehouse/userdb.db/diabets/diabetes.csv')\n",
    "df.createOrReplaceTempView('diabetes')\n",
    "df2 = sparkSession.sql('select count(*) from diabetes')\n",
    "df2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35a2ee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----+-----------------+\n",
      "| _c0|        _c1|  _c2|              _c3|\n",
      "+----+-----------+-----+-----------------+\n",
      "|1201|      Gopal|45000|Technical manager|\n",
      "|1202|    Manisha|45000|     Proof reader|\n",
      "|1203|Masthanvali|40000| Technical writer|\n",
      "|1204|      Kiran|40000|         Hr Admin|\n",
      "|1205|    Kranthi|30000|         Op Admin|\n",
      "+----+-----------+-----+-----------------+\n",
      "\n",
      "+----+-----------+------+-----------------+\n",
      "|  no|       name|salary|         position|\n",
      "+----+-----------+------+-----------------+\n",
      "|1201|      Gopal| 45000|Technical manager|\n",
      "|1202|    Manisha| 45000|     Proof reader|\n",
      "|1203|Masthanvali| 40000| Technical writer|\n",
      "|1204|      Kiran| 40000|         Hr Admin|\n",
      "|1205|    Kranthi| 30000|         Op Admin|\n",
      "+----+-----------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "saprkSession = SparkSession.builder.appName(\"pyspark-hdfs1\").getOrCreate()\n",
    "df1=sparkSession.read.csv('emp_no_header.csv')\n",
    "df1.show()\n",
    "df2=sparkSession.read.csv('emp_with_header.csv', header=True)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c58b755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------+-----------------+\n",
      "|  no|       name|salary|         position|\n",
      "+----+-----------+------+-----------------+\n",
      "|1201|      Gopal| 45000|Technical manager|\n",
      "|1202|    Manisha| 45000|     Proof reader|\n",
      "|1203|Masthanvali| 40000| Technical writer|\n",
      "|1204|      Kiran| 40000|         Hr Admin|\n",
      "|1205|    Kranthi| 30000|         Op Admin|\n",
      "+----+-----------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession.builder.appName(\"pyspark-hdfs2\").getOrCreate()\n",
    "\n",
    "#read.load() is reffered to multiple option unlimited in csv files\n",
    "df3 = sparkSession.read.load('emp_with_header.csv', \n",
    "                             format='csv', sep=',', inferSchema='true', header='true')\n",
    "# inferSchema is to define the type of data by spark-algorithm itself\n",
    "# if data is number, it can be int or if words or sentences, it can be string etc.\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be69872d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success to upload modified data which is called local-data\n",
      "+----+-----------+-------+-----------------+\n",
      "|  no|       name| salary|         position|\n",
      "+----+-----------+-------+-----------------+\n",
      "|1201|      Gopal|47250.0|Technical manager|\n",
      "|1202|    Manisha|47250.0|     Proof reader|\n",
      "|1203|Masthanvali|42000.0| Technical writer|\n",
      "|1204|      Kiran|42000.0|         Hr Admin|\n",
      "|1205|    Kranthi|31500.0|         Op Admin|\n",
      "+----+-----------+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession.builder.appName(\"reSpark-hdfs2\").getOrCreate()\n",
    "df4 = sparkSession.read.csv('emp_with_header.csv', header = 'true')\n",
    "df4 = df4.withColumn('salary', df4.salary*1.05)\n",
    "# df4.write.csv('local_data') # only one time valid\n",
    "df4.write.csv(path='local_data', mode='overwrite')\n",
    "print('success to upload modified data which is called local-data')\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c579aad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "+----+-----------+-------+-----------------+\n",
      "|  no|       name| salary|         position|\n",
      "+----+-----------+-------+-----------------+\n",
      "|1201|      Gopal|40000.0|Technical manager|\n",
      "|1202|    Manisha|40000.0|     Proof reader|\n",
      "|1203|Masthanvali|35000.0| Technical writer|\n",
      "|1204|      Kiran|35000.0|         Hr Admin|\n",
      "|1205|    Kranthi|25000.0|         Op Admin|\n",
      "+----+-----------+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkSession=SparkSession.builder.appName(\"minus\").getOrCreate()\n",
    "df5 = sparkSession.read.csv('emp_with_header.csv', header = 'true')\n",
    "df5 = df5.withColumn('salary', df5.salary-5000)\n",
    "df5.write.csv(path='hdfs://localhost:9000/user/data/csv/spark/', mode='overwrite')\n",
    "print('success')\n",
    "df5.show()\n",
    "# hdfs://localhost:9000/user/hive/w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
